{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts of Neural Networks\n",
    "---\n",
    "\n",
    "## Activation Functions\n",
    "### Reference\n",
    "- [CS231n Convolution Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-1/)\n",
    "\n",
    "### 1) Sigmoid\n",
    "- **Range**: [0, 1]\n",
    "<img src = \"http://cs231n.github.io/assets/nn1/sigmoid.jpeg\">\n",
    "- **Disadvantages**:\n",
    "    1. Saturates and Kills gradients\n",
    "        1. If activation values reaches the tails of zero or one, then the gradient becomes very close to zero.\n",
    "        2. If the gradient value is close to zero, the learning is very low and stops eventually.\n",
    "    2. Not Zero centered\n",
    "        1. This causes all the weights to be either +ve or â€“ve\n",
    "        2. Can cause over fitting/under fitting.\n",
    "\n",
    "### 2) TanH\n",
    "- **Range**: [-1, 1]\n",
    "<img src = \"http://cs231n.github.io/assets/nn1/tanh.jpeg\">\n",
    "- It is zero-centered. Thus, it is preferred over sigmoid.\n",
    "\n",
    "### 3) RELU (REctified Linear Unit)\n",
    "<img src = \"http://cs231n.github.io/assets/nn1/relu.jpeg\">\n",
    "- **Advantages:**\n",
    "    1. Faster convergence of stochastic gradient descent compared to the sigmoid/tanh functions.\n",
    "    2. Less expensive operations compared to sigmoid/tanh.\n",
    "- **Disadvantages:**\n",
    "    1. In RELU, A large gradient can update weight of a neuron in a way that it can never be activated again. It is said that if the learning rate is high, 40% of the network can be dead (never activated again).\n",
    "---\n",
    "## Momentum\n",
    "<img src = \"https://sandipanweb.files.wordpress.com/2017/11/sgd.png?w=676\">\n",
    "- As per the above image:\n",
    "    - Slower learning rate in the vertical direction\n",
    "    - Larger learning rate in the horizantal direction\n",
    "- Bowl terminology:\n",
    "    - derivatives: acceleration of ball rolling down\n",
    "    - data: friction acting to prevent speeding up of the ball above certain limits\n",
    "    - momentum: velocity of the ball rolling down\n",
    "- Usually:\n",
    "    - Gradient descent is independent of previous steps\n",
    "    - GD with momentum: it gains acceleration with taking previous gradients in consideration\n",
    "<img src = \"https://raw.githubusercontent.com/qingkaikong/blog/master/2017_05_More_on_applying_ANN/figures/figure_5.png\">\n",
    "- Momentum helps in taking large steps avoiding local optima\n",
    "- **Optimum Value:** 0.9\n",
    "\n",
    "---\n",
    "## Tuning learning rate\n",
    "### Learning rate decay:\n",
    "- reduce LR by half for every N epochs\n",
    "- exponential decay: LR = LR_Previous * EXP(- K * t ), where K: hyperparameter, t = iterations\n",
    "- 1/t decay: LR = LR_Previous / (1 + K * t)\n",
    "\n",
    "### ADAGRAD\n",
    "- cache = cache + dx ^ 2\n",
    "- x+ = - learning_rate * dx / (sqrt(cache) + eps)\n",
    "- where eps is used to avoid zero division (eps = 1e-4 to 1e-8)\n",
    "\n",
    "### RMS Prop\n",
    "- optimizes ADAGRAD by reducing the aggressive decrease in LR\n",
    "- cache = decay_rate * cache + (1 - decay_rate) * dx^2\n",
    "- x+ = - learning_rate * dx / (sqrt(cache) + eps)\n",
    "- where decay_rate is a hyper-parameter with optimal values as [0.9, 0.99, 0.999]\n",
    "\n",
    "### ADAM\n",
    "- Recently developed\n",
    "- RMS Prop + Momentum\n",
    "- m = (beta1 * m) + (1 - beta1) * dx\n",
    "- v = (beta2 * v) + (1 - beta2) * dx^2\n",
    "- x+ = - learning_rate * m / (sqrt(v) + eps)\n",
    "- where beta2 = decay_rate in RMS Prop and beta1 = momentum with optimum value as 0.9\n",
    "\n",
    "<img src = \"http://cs231n.github.io/assets/nn3/opt2.gif\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
